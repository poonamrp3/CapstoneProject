{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lb5S9D0cvCq"
   },
   "source": [
    "MLP - Classification Task - Adam + PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3HnZ9WboiXZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np, random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from deap import base, creator, tools, algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_05SClUx3qC",
    "outputId": "63679073-73cb-4716-a7da-81a19ea99262"
   },
   "outputs": [],
   "source": [
    "'''from google.colab import drive\n",
    "drive.mount('/content/drive')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "maYzkhsdZBPT"
   },
   "outputs": [],
   "source": [
    "#plot_path = \"/content/drive/MyDrive/plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywgQnX-CRmjJ"
   },
   "outputs": [],
   "source": [
    "def create_mlp(dimensions, X_train, y_train, X_test, y_test):\n",
    "    input_dim, num_hidden_layers, neurons_per_layer, output_dim = dimensions\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))\n",
    "\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(Dense(neurons_per_layer, activation='relu'))\n",
    "\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer ='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0bG8a7uotrn"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSj3VNUrEbUW"
   },
   "outputs": [],
   "source": [
    "def update_weights(model, position):\n",
    "    idx = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, Dense) and layer.trainable:\n",
    "            weights_shape = layer.kernel.shape\n",
    "            # Update weights and biases of the model\n",
    "            weights_size = np.prod(weights_shape)\n",
    "            weights = position[idx:idx+weights_size].reshape(weights_shape)\n",
    "            layer.kernel.assign(weights)\n",
    "            idx += weights_size\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTKRnIoeRCf8"
   },
   "outputs": [],
   "source": [
    "def get_weights_model(model):\n",
    "    allWeights = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, Dense) and layer.trainable:\n",
    "           weights = layer.get_weights()[0]\n",
    "           allWeights.extend(weights.flatten())\n",
    "    return allWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VC5D_TlXwyrx"
   },
   "outputs": [],
   "source": [
    "class Particle:\n",
    "      def __init__(self, position, velocity):\n",
    "          self.position = position\n",
    "          self.velocity = velocity\n",
    "          self.fitness = float('inf') #loss\n",
    "          self.accuracy = float('inf') #accuracy\n",
    "          self.local_best_position = position\n",
    "\n",
    "      def set_local_best(self, local_best_position):\n",
    "          self.local_best_position = local_best_position\n",
    "\n",
    "      def update_velocity(self, inertia_weight, cognitive_weight, social_weight, global_best_position):\n",
    "          r1, r2 = np.random.rand(2)\n",
    "          congnitive_component = cognitive_weight * r1 * (self.local_best_position - self.position)\n",
    "          social_component = social_weight * r2 * (global_best_position - self.position)\n",
    "          self.velocity = (inertia_weight * self.velocity + congnitive_component + social_component)\n",
    "          #print(\"Velocity\", self.velocity)\n",
    "\n",
    "      def update_position(self):\n",
    "          #print(\"Position\", self.position)\n",
    "          self.position += self.velocity\n",
    "\n",
    "      def evaluate_fitness(self, model, X_train, y_train, X_test, y_test):\n",
    "          # update the weights & biases of the model given the particle\n",
    "          model = update_weights(model, self.position)\n",
    "          self.fitness, self.accuracy = evaluate_model(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_BAS03ZEoh-"
   },
   "source": [
    "Update the best swarm position once fitness related to each of the particles has been calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXtW6q-WUu3_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(loss_data, accuracy_data, path):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(loss_data, label='loss', color='blue')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss', color='blue')\n",
    "    plt.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    plt.twinx()\n",
    "    plt.plot(accuracy_data, label='accuracy', color='green')\n",
    "    plt.ylabel('Accuracy', color='green')\n",
    "    plt.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "    plt.title('Loss and accuracy over the time')\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    plt.savefig(path)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTWODwH7EWsE"
   },
   "outputs": [],
   "source": [
    "def pso(n, max_trials, model, X_train, y_train, X_test, y_test,\n",
    "        inertia_weight, cognitive_weight, social_weight):\n",
    "\n",
    "    log_file='pso_log.txt'\n",
    "    # Open log file\n",
    "    with open(log_file, 'w') as f:\n",
    "        f.write(\"Starting PSO Optimization\\n\")\n",
    "        f.write(\"----------------------------------\\n\\n\")\n",
    "\n",
    "    weights_size = 0\n",
    "    for layer in model.layers:\n",
    "         weights_size += np.prod(layer.kernel.shape)\n",
    "\n",
    "    pretrained_weights_biases = get_weights_model(model)\n",
    "\n",
    "    all_particles = []\n",
    "    global_best_position = np.zeros(weights_size) #combination of weights and biases\n",
    "    global_best_fitness = float('inf') #minimum loss\n",
    "    gloval_best_accuracy = float('inf') #accuracy corresponding to the best minimum loss\n",
    "\n",
    "    local_best_position = np.zeros(weights_size) #combination of weights and biases\n",
    "    local_best_fitness = float('inf') #minimum loss\n",
    "    local_best_accuracy = float('inf') #accuracy corresponding to the best minimum loss\n",
    "    noise_range = 0.01  # Initial noise range\n",
    "    min_val = -10.0\n",
    "    #max_val = 0.1\n",
    "    max_val = 10.0\n",
    "    tempLow = min_val * 0.01\n",
    "    tempHigh = max_val * 0.01\n",
    "    \n",
    "    for _ in range(n):\n",
    "        position = pretrained_weights_biases + np.random.uniform(tempLow, tempHigh, size=weights_size)\n",
    "        velocity = np.random.uniform(tempLow, tempHigh, size=weights_size)\n",
    "\n",
    "        particle = Particle(position, velocity)\n",
    "        particle.update_position()\n",
    "        particle.evaluate_fitness(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "        if particle.fitness < global_best_fitness:\n",
    "           local_best_fitness = particle.fitness\n",
    "           local_best_position = particle.position\n",
    "           local_best_accuracy = particle.accuracy\n",
    "\n",
    "           global_best_fitness = particle.fitness\n",
    "           global_best_position = particle.position\n",
    "           global_best_accuracy = particle.accuracy\n",
    "\n",
    "        all_particles.append(particle)\n",
    "\n",
    "    # Initialize min_loss and max_loss\n",
    "    min_loss = float('inf')  # Start with a very high value\n",
    "    max_loss = float('-inf')  # Start with a very low value\n",
    "    # Initialize min_ind and max_ind\n",
    "    min_ind = -1\n",
    "    max_ind = -1\n",
    "\n",
    "    # Iterate through all particles\n",
    "    for i in range(len(all_particles)):\n",
    "        current_fitness = all_particles[i].fitness\n",
    "\n",
    "        # Update min_loss and min_ind\n",
    "        if current_fitness < min_loss:\n",
    "            min_loss = current_fitness\n",
    "            min_ind = i\n",
    "        # Update max_loss and max_ind\n",
    "        if current_fitness > max_loss:\n",
    "            max_loss = current_fitness\n",
    "            max_ind = i\n",
    "\n",
    "    min_loss_data = []\n",
    "    min_acc_data = []\n",
    "    max_loss_data = []\n",
    "    max_acc_data= []\n",
    "\n",
    "    print(f\"Best Global Loss/Fitness = {global_best_fitness}, Best Accuracy = {global_best_accuracy}\")\n",
    "    print(f\"Best Local Loss/Fitness = {local_best_fitness}, Best Accuracy = {local_best_accuracy}\")\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    tempLow = min_val * 0.0001\n",
    "    tempHigh = max_val * 0.0001\n",
    "    tol = 5\n",
    "    patience = 0\n",
    "    isEnd = False\n",
    "    best_fitness_history = [global_best_fitness]\n",
    "    for trial in range(max_trials):\n",
    "        inertia_weight = inertia_weight * 0.99\n",
    "        for i in range(n):\n",
    "            current_particle = all_particles[i]\n",
    "            current_particle.update_velocity(inertia_weight, cognitive_weight, social_weight, global_best_position)\n",
    "            current_particle.velocity = np.clip(current_particle.velocity, tempLow, tempHigh)\n",
    "\n",
    "            current_particle.update_position()\n",
    "            current_particle.evaluate_fitness(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "            if i == min_ind:\n",
    "               min_loss_data.append(current_particle.fitness)\n",
    "               min_acc_data.append(current_particle.accuracy)\n",
    "\n",
    "            if i == max_ind:\n",
    "               max_loss_data.append(current_particle.fitness)\n",
    "               max_acc_data.append(current_particle.accuracy)\n",
    "\n",
    "            if current_particle.fitness < local_best_fitness:\n",
    "               local_best_fitness = current_particle.fitness\n",
    "               local_best_position = current_particle.position\n",
    "               local_best_accuracy = current_particle.accuracy\n",
    "               current_particle.set_local_best(local_best_position)\n",
    "\n",
    "            if current_particle.fitness < global_best_fitness:\n",
    "                global_best_fitness = current_particle.fitness\n",
    "                global_best_position = current_particle.position\n",
    "                global_best_accuracy = current_particle.accuracy\n",
    "                patience = 0\n",
    "                best_fitness_history.append(global_best_fitness)\n",
    "\n",
    "        if global_best_fitness >= min(best_fitness_history):\n",
    "           patience += 1\n",
    "            \n",
    "        if patience >= tol:\n",
    "           message = f\"\\nEarly stopping at trial {trial+1} - No improvement for {patience} consecutive trials.\"\n",
    "           print(message)\n",
    "           with open(log_file, 'a') as f:\n",
    "                f.write(message + \"\\n\")\n",
    "           break\n",
    "        else:\n",
    "          message = f\"Trial {trial+1}: Best Loss/Fitness PSO = {global_best_fitness}, Best Accuracy = {global_best_accuracy}\"\n",
    "          print(message)\n",
    "          with open(log_file, 'a') as f:\n",
    "              f.write(message + \"\\n\")\n",
    "              f.write(\"-----------------------------------------------------------------------------------------------\" + \"\\n\")\n",
    "        \n",
    "\n",
    "    path = 'plots/bcancer_perform_best_100.png'\n",
    "    plot_data(min_loss_data, min_acc_data, path)\n",
    "    path = 'plots/bcancer_perform_worst_100.png'\n",
    "    plot_data(max_loss_data, max_acc_data, path)\n",
    "    return global_best_position, global_best_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIOu5-1I7LNe"
   },
   "outputs": [],
   "source": [
    "# PSO parameters\n",
    "n_particles = 30\n",
    "max_trials = 100\n",
    "w = 0.7  # Inertia weight\n",
    "c1 = 1.5  # Cognitive parameter\n",
    "c2 = 1.5  # Social parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaWOyp23hewE"
   },
   "outputs": [],
   "source": [
    "def setup_run(X_train, X_test, y_train, y_test, dimensions, inertia_weight, cognitive_weight, social_weight):\n",
    "    # Create a base model\n",
    "    print(\"Before PSO\")\n",
    "    model = create_mlp(dimensions, X_train, y_train, X_test, y_test)\n",
    "    evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "    print(\"Now PSO estimations\")\n",
    "    pso(n_particles, max_trials, model, X_train, y_train, X_test, y_test, inertia_weight, cognitive_weight, social_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MNYgdPcj1aR"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbV2_qP9hU2x",
    "outputId": "2cfee19e-0ec7-4d93-d918-be94e3dbe44d"
   },
   "outputs": [],
   "source": [
    "## Load dataset and run\n",
    "def load_and_run(dataset_loader):\n",
    "    data = dataset_loader()\n",
    "    X = data.data\n",
    "    y = data.target.reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    encoder = OneHotEncoder()\n",
    "    y = encoder.fit_transform(y).toarray()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Dynamically setting input shape\n",
    "    input_shape = X_train.shape[1]\n",
    "    num_hidden_layers = 3\n",
    "    neurons_per_layer = 5\n",
    "    output_dim = y_train.shape[1]\n",
    "    dimensions = input_shape, num_hidden_layers, neurons_per_layer, output_dim\n",
    "    print(dimensions)\n",
    "    setup_run(X_train, X_test, y_train, y_test, dimensions, w, c1, c2)\n",
    "\n",
    "#usage\n",
    "#load_and_run(load_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rVFpo8pjKmjZ",
    "outputId": "0370f9c4-2aa6-42fa-9733-84db75b1c566"
   },
   "outputs": [],
   "source": [
    "'''from sklearn.datasets import load_wine\n",
    "data = load_wine()\n",
    "X, y = data.data, data.target\n",
    "load_and_run(load_wine)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7vAKhmiKo6C",
    "outputId": "415475dd-ec56-40cc-c1c7-70c0222914e2"
   },
   "outputs": [],
   "source": [
    "'''from sklearn.datasets import load_digits\n",
    "data = load_digits()\n",
    "X, y = data.data, data.target\n",
    "load_and_run(load_digits)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6Y3SADtakqn",
    "outputId": "7d9c075a-9183-4280-d52f-71092b99ba18"
   },
   "outputs": [],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IJhxPcFdalNv",
    "outputId": "aeab5810-79f3-4f13-9c40-adaa846eb766"
   },
   "outputs": [],
   "source": [
    "'''from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = breast_cancer_wisconsin_diagnostic.data.features\n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets\n",
    "\n",
    "# metadata\n",
    "# print(breast_cancer_wisconsin_diagnostic.metadata)\n",
    "\n",
    "# variable information\n",
    "# print(breast_cancer_wisconsin_diagnostic.variables)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "encoder = OneHotEncoder()\n",
    "y = encoder.fit_transform(y).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dynamically setting input shape\n",
    "input_shape = X_train.shape[1]\n",
    "num_hidden_layers = 3\n",
    "neurons_per_layer = 5\n",
    "output_dim = y_train.shape[1]\n",
    "dimensions = input_shape, num_hidden_layers, neurons_per_layer, output_dim\n",
    "setup_run(X_train, X_test, y_train, y_test, dimensions, w, c1, c2)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "spambase = fetch_ucirepo(id=94) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = spambase.data.features \n",
    "y = spambase.data.targets \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "encoder = OneHotEncoder()\n",
    "y = encoder.fit_transform(y).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dynamically setting input shape\n",
    "input_shape = X_train.shape[1]\n",
    "num_hidden_layers = 3\n",
    "neurons_per_layer = 5\n",
    "output_dim = y_train.shape[1]\n",
    "dimensions = input_shape, num_hidden_layers, neurons_per_layer, output_dim\n",
    "setup_run(X_train, X_test, y_train, y_test, dimensions, w, c1, c2)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
